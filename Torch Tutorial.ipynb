{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ezOadVeyyq2"
   },
   "source": [
    "# ðŸ”¥ Intro to PyTorch ðŸ”¥\n",
    "\n",
    "Table of Contents:\n",
    "\n",
    "0. Introduction\n",
    "1. Torch Tensors\n",
    "    1. Basics\n",
    "    2. Using the GPU\n",
    "    3. Math\n",
    "2. Autograd\n",
    "3. Neural networks in Torch\n",
    "\n",
    "\n",
    "## 0. Introduction\n",
    "\n",
    "In this tutorial, we'll walk you through the basics of using `torch`.\n",
    "\n",
    "Note: we'll use PyTorch and Torch interchangeably.\n",
    "\n",
    "In many ways, you can think of it as a souped-up `numpy` with a few key differences:\n",
    "1. You can use `torch` with a GPU for super fast matrix calculations.\n",
    "2. Torch provides _automatic differentiation_.\n",
    "3. Torch provides some structure for building neural networks.\n",
    "\n",
    "This notebook is adapted from [this tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), which you can consult as a more in-depth guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7teSC4pEyyq7"
   },
   "outputs": [],
   "source": [
    "# first, let's import torch and some other libraries\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQjH1J0Cyyq8"
   },
   "source": [
    "## 1. Torch Tensors: `np.array`'s cool older sibling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bYRRs4Syyq9"
   },
   "source": [
    "Machine learning requires a lot of linear algebra. So all of these libraries (`torch`, `numpy`, `tensorflow`, etc), at their most basic level, are just libraries to aid you (and the computer) in doing linear algebraic operations, quickly.\n",
    "\n",
    "It makes sense, then, that the basic data structure is the `torch.Tensor`, which is essentially a GPU-enabled `np.array`.\n",
    "\n",
    "<!-- Tensors are always associated with a memory location. This has always been true, of course, but generally Python abstracts that all away for us. It's worth being explicit now, because you will have different memory locations when moving tensors between the GPU and CPU. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMJupj2Zyyq9"
   },
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqCHVAotyyq-",
    "outputId": "cab4abee-4cf2-49fe-ac4a-d890fadf3462"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.1704e-41],\n",
       "        [0.0000e+00, 2.2369e+08, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [       nan,        nan, 1.2783e+33]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uninitialized 5x3 matrix\n",
    "# values are undefined when matrix is uninitialized (note that this is not the same as matrix of zeros)\n",
    "\n",
    "torch.empty(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9VU8_9Oyyq_",
    "outputId": "f33918d9-d85f-4bd8-d580-3dc42f956d73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1788, 0.2140, 0.3977],\n",
       "        [0.3545, 0.8557, 0.3537],\n",
       "        [0.1531, 0.4902, 0.5251],\n",
       "        [0.9243, 0.0865, 0.2306],\n",
       "        [0.5489, 0.1476, 0.3132]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly initialized (uniform on [0,1))\n",
    "\n",
    "torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnLYyd_lyyrA",
    "outputId": "b1d49db3-790f-4386-c851-0349fae74e4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5000, 3.0000],\n",
       "        [4.0000, 1.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we can initialize straight from data, just like numpy\n",
    "\n",
    "x = torch.tensor([[5.5, 3], [4, 1]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XvE-CvkyyrB"
   },
   "source": [
    "Just like numpy arrays, tensors are multidimensional and homogeneously typed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igh6W5SkyyrB",
    "outputId": "c552e4db-259d-48db-95c8-dfaa1f4d5c87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.Size is a tuple\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAQCLoE7yyrC",
    "outputId": "7e24e4ed-764c-4e59-db69-f29bcfbad45f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m74cIGPYyyrC"
   },
   "source": [
    "In fact, if you're on the CPU, you can very easily convert from tensors to nparrays and back again. They will point to the same memory location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOKRdzt7yyrC",
    "outputId": "434a334b-2708-4790-b252-a904db603a58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.5, 3. ],\n",
       "       [4. , 1. ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wow!\n",
    "a = x.numpy()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niqQZPtfyyrD",
    "outputId": "6a042f26-729a-4e63-a53b-730591b46aa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.5000, 4.0000],\n",
       "        [5.0000, 2.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add in place (+= 1)\n",
    "x.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4XqUnmdyyrD",
    "outputId": "eca462ac-1833-483d-95b0-2ab6884e12b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.5, 4. ],\n",
       "       [5. , 2. ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGseexDjyyrE"
   },
   "source": [
    "### Using the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhAgEXSyyyrE"
   },
   "outputs": [],
   "source": [
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "# The if statement checks if you have a GPU available\n",
    "# This only works for NVIDIA graphics cards\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9N5-U1qsyyrE"
   },
   "source": [
    "### Math with Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg1QzSySyyrE"
   },
   "source": [
    "Like we alluded to earlier, we can do math with tensors.\n",
    "\n",
    "There are many, many ways to do the same thing in Torch. To see (almost) all of them, see [here](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#operations). I'm just going to list the basics, but I recommend checking them all out so you know what to expect when reading other people's code.\n",
    "\n",
    "As just one example, here's how we can do addition, but you can check the [docs](https://pytorch.org/docs/stable/torch.html#math-operations) for all the math operations available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NB7mvKlcyyrF"
   },
   "outputs": [],
   "source": [
    "y = torch.rand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiXEkBHkyyrF",
    "outputId": "2053e49a-cb99-4263-dac1-0fe3053ab982"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.3992, 4.7970],\n",
       "        [5.3645, 2.8396]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it really is that easy\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kM0c-5kDyyrF",
    "outputId": "71fe73c6-6bcf-4429-a109-f1187e652680"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.3992, 4.7970],\n",
       "        [5.3645, 2.8396]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBHUj_DAyyrG"
   },
   "source": [
    "## 2. Autograd: calc III in a python module\n",
    "\n",
    "Every tensor tracks the operations performed on it, so Torch can automatically compute the gradient with respect to that tensor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p_-w_4hyyrG"
   },
   "source": [
    "Consider this familiar looking set of equations:\n",
    "\n",
    "$$\n",
    "z = \\bar{\\theta} \\cdot \\bar{x} + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = \\max({0, 1 - yz})\n",
    "$$\n",
    "\n",
    "where $y$ is the target output and $z$ is the output of the linear classifier.\n",
    "\n",
    "We want to minimize loss with respect to $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swcifrhwyyrG"
   },
   "outputs": [],
   "source": [
    "# let's define some values for our vectors.\n",
    "# we use requires_grad to tell Torch to track operations on these tensors\n",
    "# we initialize b to a Torch scalar, and also track operations on it\n",
    "\n",
    "theta = torch.tensor([1., 2.], requires_grad=True)\n",
    "x = torch.tensor([3., 1.], requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "z = torch.dot(theta, x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-FAaPT6yyrG"
   },
   "source": [
    "Now let's introduce our loss function,  hinge loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbsSZApJyyrH"
   },
   "outputs": [],
   "source": [
    "y = -1  # this is our target value\n",
    "loss = 1 - y * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpezZm1tyyrH",
    "outputId": "2704770f-28a6-4cb1-b03d-f43342c09a89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-Mf1NY5yyrH"
   },
   "source": [
    "Now, we propagate backwards from the loss to get $ \\nabla_{}L$, the partial of the loss with respect to each of the variables. This updates all of the tensors which go into the calculation for loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bw-rL4GyyrI"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxkciDmvyyrI"
   },
   "source": [
    "Finally, to find $ \\nabla_{\\bar{\\theta}}L$, we access the `.grad` attribute of our `theta` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTpGmXQoyyrI",
    "outputId": "48bb7c20-02be-4874-f9cc-3b473ec02d9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_rH3qIEyyrI"
   },
   "source": [
    "Incredible!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/PUBxelwT57jsQ/source.gif\" />\n",
    "\n",
    "Important note: gradients _accumulate_, so you probably want to zero out all of the gradients before. For example, let's see what happens if we run the computations again (starting from `y = torch.dot(...`)\n",
    "\n",
    "You can read more about autograd [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py) (the tutorial) and a lot more [here](https://pytorch.org/docs/stable/autograd.html) (the documentation) and a lot, lot more [here](https://openreview.net/pdf/25b8eee6c373d48b84e5e9c6e10e7cbbbce4ac73.pdf) (the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skNbCCP2yyrI",
    "outputId": "5a51515a-5d33-41f4-d176-e89fe9137102"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb6wUjejyyrJ"
   },
   "source": [
    "## 3. Neural Networks with Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PCM3z0PyyrJ"
   },
   "source": [
    "Now, let's write a neural network together in PyTorch. It's easy and fun!\n",
    "\n",
    "We'll start with some concepts:\n",
    "\n",
    "Torch provides some nice helper functions and code to help us train our neural nets. This ranges from code to load data (`torch.utils.data`) to a variety of activation functions and hidden layers (`torch.nn`). This also means for our code to play nicely with the rest of Torch, we want to follow the same structure as the rest of the Torch codebase.\n",
    "\n",
    "Enter `torch.nn.Module` (we will abbreviate to `nn.Module`). You can think of `nn.Module` as single \"unit\" of your architecture. It's anything with layers, an input, and `forward()` function which computes output. For simple models, it might be your entire model. For more complex models, it might be a logical, resuable component (e.g. a large language model that you then fine-tune over).\n",
    "\n",
    "`torch.nn` uses autograd to define and differentiate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gCkcTXgyyrJ"
   },
   "source": [
    "### General Outline\n",
    "\n",
    "In most cases, these are the steps you take to train a neural network:\n",
    "\n",
    "0. Define the neural network architecture with some learnable parameters (or weights)\n",
    "1. Iterate over a dataset of inputs\n",
    "2. Process input through the network\n",
    "3. Compute the loss (how far is the output from being correct)\n",
    "4. Propagate gradients back into the networkâ€™s parameters\n",
    "5. Update the weights of the network\n",
    "\n",
    "We will work with an example dataset trying to classify between 10 characters of a cursive Japanese phonetic writing system: Kuzushiji. You can think of this as MNIST, but cooler.\n",
    "\n",
    "![Image of KMNIST dataset](https://github.com/rois-codh/kmnist/raw/master/images/kmnist_examples.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3QmJtdcyyrK"
   },
   "outputs": [],
   "source": [
    "# first, we download and load the dataset\n",
    "# if you're running for the first time, it might take a minute to download\n",
    "import torchvision\n",
    "root = \"./mnist/\"\n",
    "dataset = torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XByPpxeyyrK",
    "outputId": "e8ca6295-a8de-4afa-ed2d-d24009f9af02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./mnist/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cPgfrPfyyrK"
   },
   "source": [
    "Let's see what a single example from the dataset looks like. A 28x28 matrix. We will flatten this to a single 784x1 vector for our toy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nV8iXdrayyrK",
    "outputId": "e3d5a71b-4430-44d6-f02f-0b56f7d81001",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = dataset[0]\n",
    "np.array(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq6v2QZ8yyrK"
   },
   "source": [
    "Great, let's start making our classifier!\n",
    "\n",
    "### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4VDaVz6yyrL"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVa1FcIXyyrL"
   },
   "outputs": [],
   "source": [
    "# A module is just something with:\n",
    "\n",
    "class CoolClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CoolClassifier, self).__init__()\n",
    "        # 1. layers\n",
    "\n",
    "    # 2. a function that takes an input\n",
    "    def forward(self, x):\n",
    "        # 3. and that returns an output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM-mP9c4yyrL"
   },
   "outputs": [],
   "source": [
    "class CoolClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CoolClassifier, self).__init__()\n",
    "        self.hidden1 = nn.Linear(784, 200)\n",
    "        self.hidden2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.hidden2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38YU15MlyyrL",
    "outputId": "85d2199a-477d-4c1b-e445-4b8f220fb4fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoolClassifier(\n",
       "  (hidden1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (hidden2): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CoolClassifier()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd6oxGJ8yyrL",
    "outputId": "3402bb98-f6cc-41e8-884b-59fb0410298e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 784])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the first layer?\n",
    "list(net.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "We9U7sdcyyrL"
   },
   "source": [
    "Cool. We have our model (with totally useless weights, of course). Let's forward propagate (but remember to zero out the gradients first.)\n",
    "\n",
    "### Process an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbp_AKL-yyrM",
    "outputId": "cb4c643b-44ac-4f1d-f639-81d36687b49c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4.6051,  -0.5493, -19.5514,   6.2472, -13.4110,  -3.7954, -18.7314,\n",
       "         -23.9434, -13.7989,   5.8312]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = dataset[0]\n",
    "X = np.array(X).flatten()  # convert 28x28 to 784x1\n",
    "X = torch.tensor(X, dtype=torch.float32)  # convert int nparray to float tensor\n",
    "X = X.unsqueeze(0)  # fake a minibatch of size 1\n",
    "\n",
    "y = torch.tensor(y).unsqueeze(0)\n",
    "\n",
    "net.zero_grad()\n",
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWIEirhsyyrM",
    "outputId": "ecbd133f-b5c3-4032-d151-15e70a401dc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMauelHqyyrM"
   },
   "source": [
    "As an aside, we learned before that it makes the most sense to use minibatch gradient descent, and this is what Torch was designed for. That means we can only feed in inputs with shape `[batch_size, input_shape]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5SEYnmNyyrM"
   },
   "source": [
    "Okay, great, now we just need to teach our model. To compute the loss, we'll use Cross Entropy Loss, since we're doing a classification task.\n",
    "\n",
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j27jZKr-yyrM",
    "outputId": "c8071ebc-c9f2-4c8a-bee8-ffe65c4a2fa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.6601, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvfzYjMEyyrM"
   },
   "source": [
    "Now we can differentiate w.r.t. the loss (backpropagate!)\n",
    "\n",
    "### Propagate gradients back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6qeyoIQyyrM"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2HxLL3OyyrN",
    "outputId": "81eeac31-5244-400f-e1b8-3c9043c235ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0439e-01,  6.0272e-04,  3.3698e-12,  5.3924e-01,  1.5644e-09,\n",
       "        -9.9998e-01,  7.6511e-12,  4.1703e-14,  1.0614e-09,  3.5574e-01])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.hidden2.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGmTga43yyrN"
   },
   "source": [
    "### Update weights\n",
    "\n",
    "The simplest way to do this is with SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mufIKJByyrN"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    # for each parameter, subtract the gradient * learning rate\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPkWwysTyyrN"
   },
   "source": [
    "However, scientists have spent lots of time inventing very sophisticated ways of updating the weights beyond just SGD. Torch includes many of these more advanced update rules, called optimizers, in the `torch.optim` module.\n",
    "\n",
    "As an example, here's how to use the built-in version of SGD, but we might prefer something like Adam instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWtMsJN9yyrN"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(X)\n",
    "loss = criterion(output, y)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0bwt8OCyyrN"
   },
   "source": [
    "### Putting it all together\n",
    "\n",
    "All of these parts go together to create our training procedure, which should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_3VDvo8yyrN"
   },
   "outputs": [],
   "source": [
    "train, val = torch.utils.data.random_split(dataset, [50_000, 10_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivD6fUdTyyrO",
    "outputId": "fcb0a76a-7a77-4d66-9d86-af5dd3d7ee1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss 21429.099609375\n",
      "Training batch loss 370.7990417480469\n",
      "Training batch loss 10.451495170593262\n",
      "Training batch loss 4.543331146240234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy of 0.8579000234603882\n",
      "Training batch loss 0.9304068684577942\n",
      "Training batch loss 20.528350830078125\n",
      "Training batch loss 1.0830923318862915\n",
      "Training batch loss 2.068761110305786\n",
      "Val accuracy of 0.9032999873161316\n",
      "Training batch loss 3.31925892829895\n",
      "Training batch loss 6.0038886070251465\n",
      "Training batch loss 0.0\n",
      "Training batch loss 0.000250410899752751\n",
      "Val accuracy of 0.9013000130653381\n",
      "Training batch loss 4.18876952608116e-05\n",
      "Training batch loss 0.0003012671077158302\n",
      "Training batch loss 30.823930740356445\n",
      "Training batch loss 13.435836791992188\n",
      "Val accuracy of 0.9045000076293945\n",
      "Training batch loss 5.446247100830078\n",
      "Training batch loss 0.0\n",
      "Training batch loss 15.253301620483398\n",
      "Training batch loss 1.4676566123962402\n",
      "Val accuracy of 0.9291999936103821\n"
     ]
    }
   ],
   "source": [
    "# See also: https://pytorch.org/tutorials/beginner/nn_tutorial.html#add-validation\n",
    "# This might take a few minutes to run.\n",
    "\n",
    "# From before,\n",
    "# 1. Define the model, then instantiate the model\n",
    "# net = CoolClassifier()\n",
    "\n",
    "# The DataLoaders help us create and iterate over minibatches\n",
    "# We use the collate_fn to do our preprocessing for each batch.\n",
    "# i.e. convert each image into a 784x1 tensor\n",
    "def collate_fn(batch):\n",
    "    X = torch.tensor([np.array(x).flatten() for x, _ in batch], dtype=torch.float32)\n",
    "    y = torch.tensor([y for _, y in batch])\n",
    "    return X, y\n",
    "\n",
    "batch_size = 16\n",
    "train_dl = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = torch.utils.data.DataLoader(val, batch_size=batch_size * 2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Create our optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # set training mode\n",
    "    # this affects things like dropout or batch normalization\n",
    "    net.train()\n",
    "    for i, (X, y) in enumerate(train_dl):\n",
    "        output = net(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Training batch loss {loss}\")\n",
    "    # set in evaluation mode\n",
    "    net.eval()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    for X, y in val_dl:\n",
    "        output = net(X)\n",
    "        prediction = F.softmax(output)\n",
    "        num_correct += torch.sum(torch.argmax(prediction, dim=1) == y)\n",
    "        num_total += len(X)\n",
    "    print(f\"Val accuracy of {num_correct.true_divide(num_total)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Torch Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
